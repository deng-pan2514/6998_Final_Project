# 6889_Final_Project

## Description

The project aims to implement, profile speculative decoding and study how kv cache and selection of draft model impact its performance.
## Outline
autoregressive_sampling.py: implemented autoregressive sampling as baseline
kv_cache.py: implemented kv cache optimization
speculative_sampling: implemented speculative decoding. you can revise the model selection and prompt in this file.
demo.py: defined main function. you can run this demo to observe the result of speculative decoding

## Commands to run
Read requirements for environment setup

Run the demo with python demo.py
There are default prompt and model selection in demo.py which you can revise.

## Results

Due to some network issue I can't upload the results images. Please check it in the report.
KV cache greatly improves performance and the impact is larger on large target model. Speculative decoding brings 2x speedup. Llama 7B is the best choice in my experiment Throughput of 1.3B and 2.7B are quite low. It may be due to low acceptance rate of tokens generated by them so large model runs multiple times to correct.
